# scope: user
# meta developer: @Hikimuro
# ver: 1.0.7

from .. import loader, utils
import cloudscraper
import random
import logging
from bs4 import BeautifulSoup
import aiohttp
import os

logger = logging.getLogger(__name__)

@loader.tds
class FapReactorMod(loader.Module):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–ª—É—á–∞–π–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å fapreactor.com –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""

    strings = {
        "name": "FapReactor",
        "no_category": "‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π .setfapcategory <–∫–∞—Ç–µ–≥–æ—Ä–∏—è>",
        "not_found": "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n\n–ü—Ä–∏—á–∏–Ω–∞: {}",
        "downloading": "üîç –ò—â—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ..."
    }

    def __init__(self):
        self.config = loader.ModuleConfig(
            loader.ConfigValue(
                "category", 
                None, 
                lambda: "–†–∞–∑–¥–µ–ª —Å fapreactor.com (–Ω–∞–ø—Ä–∏–º–µ—Ä: hentai, porn –∏ —Ç.–¥.)"
            )
        )
        self.scraper = cloudscraper.create_scraper()

    @loader.command()
    async def setfapcategory(self, message):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é (—Ä–∞–∑–¥–µ–ª)"""
        args = utils.get_args_raw(message)
        if not args:
            await message.edit("‚ö†Ô∏è –£–∫–∞–∂–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é.")
            return
        self.set("category", args)
        await message.edit(f"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –Ω–∞: `{args}`")

    @loader.command()
    async def fap(self, message):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞–Ω–¥–æ–º–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å fapreactor.com"""
        category = self.get("category")
        if not category:
            await message.edit(self.strings("no_category"))
            return

        await message.edit(self.strings("downloading"))

        try:
            for _ in range(5):  # –¥–æ 5 –ø–æ–ø—ã—Ç–æ–∫
                page = random.randint(1, 10)
                url = f"https://fapreactor.com/tag/{category}/all/{page}"
                r = self.scraper.get(url, timeout=10)
                r.raise_for_status()
                soup = BeautifulSoup(r.text, "html.parser")
                posts = soup.select("div.image > a > img")
                if posts:
                    break
            else:
                raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ—Å–ª–µ 5 –ø–æ–ø—ã—Ç–æ–∫.")

            images = [img["src"] for img in posts if "src" in img.attrs]
            image_url = random.choice(images)
            if image_url.startswith("//"):
                image_url = "https:" + image_url

            temp_file = "fapreactor_image.jpg"

            headers = {
                "User-Agent": "Mozilla/5.0",
                "Referer": "https://fapreactor.com/"
            }

            async with aiohttp.ClientSession(headers=headers) as session:
                async with session.get(image_url) as resp:
                    if resp.status != 200:
                        raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {resp.status}")
                    data = await resp.read()
                    with open(temp_file, "wb") as f:
                        f.write(data)

            await message.client.send_file(message.chat_id, temp_file)
            await message.delete()
            os.remove(temp_file)

        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å fapreactor")
            await message.edit(self.strings("not_found").format(str(e)))
