# scope: user
# meta developer: @Hikimuro
# ver: 2.0.2

from .. import loader, utils
import cloudscraper
import random
import logging
from bs4 import BeautifulSoup
import aiohttp
import os

logger = logging.getLogger(__name__)

AVAILABLE_CATEGORIES = [
    "Oral", "Vaginal", "Sex Toys", "Group",
    "Lesbian", "Handjob", "Futanari", "Porn", "Hentai",
    "Shemale", "Newhalf", "Footfetish", "Femdom", "Milf",
    "Anal", "Teen", "Bdsm", "Yaoi", "Yuri", "Guro"
]

@loader.tds
class FapReactorMod(loader.Module):
    """
FapReactor:
 –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–ª—É—á–∞–π–Ω–æ–µ NSFW –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å fapreactor.com –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    """

    strings = {
        "name": "FapReactor",
        "no_category": "‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π .setfapcategory <–∫–∞—Ç–µ–≥–æ—Ä–∏—è>",
        "not_found": "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n\n–ü—Ä–∏—á–∏–Ω–∞: {}",
        "downloading": "üîç –ò—â—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ..."
    }

    def __init__(self):
        self.config = loader.ModuleConfig(
            loader.ConfigValue(
                "category",
                "Porn",
                lambda: "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å fapreactor.com (–¥–æ—Å—Ç—É–ø–Ω—ã–µ: {})".format(", ".join(AVAILABLE_CATEGORIES))
            ),
            loader.ConfigValue(
                "proxy",
                None,
                lambda: "–ü—Ä–æ–∫—Å–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ http://user:pass@ip:port –∏–ª–∏ http://ip:port"
            )
        )

    def _init_scraper(self):
        scraper = cloudscraper.create_scraper(
            browser={'custom': 'ScraperBot/1.0'},
            interpreter='nodejs',
            delay=10
        )
        proxy = self.config.get("proxy")
        if proxy:
            scraper.proxies.update({
                "http": proxy,
                "https": proxy
            })
        return scraper

    @loader.command()
    async def setfapcategory(self, message):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é (—Ä–∞–∑–¥–µ–ª)"""
        args = utils.get_args_raw(message)
        if not args:
            await utils.answer(message, "‚ö†Ô∏è –£–∫–∞–∂–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é.\n–î–æ—Å—Ç—É–ø–Ω—ã–µ: " + ", ".join(AVAILABLE_CATEGORIES))
            return
        if args not in AVAILABLE_CATEGORIES:
            await utils.answer(message, f"‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏—è `{args}` –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.\n–î–æ—Å—Ç—É–ø–Ω—ã–µ: {', '.join(AVAILABLE_CATEGORIES)}")
            return
        self.config["category"] = args
        await utils.answer(message, f"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –Ω–∞: `{args}`")

    @loader.command()
    async def fap(self, message):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞–Ω–¥–æ–º–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å fapreactor.com"""
        category = self.config["category"]
        if not category:
            await utils.answer(message, self.strings("no_category"))
            return

        await utils.answer(message, self.strings("downloading"))

        try:
            scraper = self._init_scraper()
            for _ in range(5):
                page = random.randint(1, 1000)
                url = f"https://fapreactor.com/tag/{category}/all/{page}"
                r = scraper.get(url, timeout=10)
                r.raise_for_status()
                soup = BeautifulSoup(r.text, "html.parser")
                posts = soup.select("div.image > a > img")
                if posts:
                    break
            else:
                raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ—Å–ª–µ 5 –ø–æ–ø—ã—Ç–æ–∫.")

            images = [img["src"] for img in posts if "src" in img.attrs]
            image_url = random.choice(images)
            if image_url.startswith("//"):
                image_url = "https:" + image_url

            temp_file = "fapreactor_image.jpg"

            headers = {
                "User-Agent": "Mozilla/5.0",
                "Referer": "https://fapreactor.com/"
            }

            proxy = self.config.get("proxy")

            async with aiohttp.ClientSession(headers=headers) as session:
                async with session.get(image_url, proxy=proxy) as resp:
                    if resp.status != 200:
                        raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {resp.status}")
                    data = await resp.read()
                    with open(temp_file, "wb") as f:
                        f.write(data)

            await message.client.send_file(message.chat_id, temp_file)
            await message.delete()
            os.remove(temp_file)

        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å fapreactor")
            await utils.answer(message, self.strings("not_found").format(str(e)))
